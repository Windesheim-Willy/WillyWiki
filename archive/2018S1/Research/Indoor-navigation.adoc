include::../../../Header.adoc[]
=== Indoor-navigation

[discrete]
==== Version history


[cols=",,,",options="header",]
|============================================
|Version |Date |Person |Note
|V0.1 |04-03-18 |Jesse Bouwman |
|V0.2 |14-03-18 |Jonathan ten Hove |Review
|V0.3 |15-03-18 |Jesse Bouwman |Minor changes
|============================================

==== Preface


Before Willy can drive autonomic inside, Willy must be aware of his
environment. Because GPS is not available indoors, another solution must
be available. Autonomic driving will be supported by the current sensors
on Willy such as the ultrasonic sensors and this research focusses on a
separate technology that makes Willy more aware of his position.

==== Goal of indoor navigation

The goal of indoor navigation is that Willy can drive autonomously
within a building where GPS is not available and be more aware of its
surroundings. Willy needs a system which can recognize where it is
located indoors. With this system Willy can drive indoors, follow a
route or drive in a way that the coverage of the area is as high as
possible for a detailed map and cleaning.

==== Compatibility requirements

The indoor navigation system needs to work with the other systems. Willy
uses ultrasonic sensors to avoid obstacles. The ultrasonic sensors need
to have priority above the navigation so Willy can never hit an
obstacle. If the navigation needs to get somewhere behind an obstacle it
needs to find an alternative route.

Another requirement is that when Willy is outside, GPS takes over the
function of positioning. However the systems that work inside could help
the ultrasonic sensors with avoiding obstacles.

Challenge of implementing indoor navigation

There are some challenges with implementing indoor navigation. In the
first place we need the sensors. At the time of writing Willy has a
Kinect as well as a SICK TIM-551 LiDAR available. However both are not
implemented as of this moment.

Another challenge is that the system of Willy is already quite
complicated because of the different modules. The benefit of the
ROS-platform is that a new module could easily be installed. The
communication goes via ROS where other modules are connected. However
there is no way at this moment to connect new modules hardware-based.
There are no open connections left for additional hardware.

=== Research

=== Different solutions

==== Kinect

A Kinect version 1 sensor has previously been used on Willy. It was used
to recognize objects and people. Unfortunately the sensor is currently
not implemented in Willy.

The Kinect could be used again and has a few very important features. A
research of the previous group showed us that there is a newer version
with better features. A problem with the newer version is that the
Kinect V2 is not documented yet. This means that if we want to use the
newer version we need to write it all ourselves. When using Kinect V1,
almost everything we need is available. Another reason is that the newer
version is a lot more expensive.

[cols=",,",options="header",]
|======================================================
|Feature |Kinect 1 |Kinect 2
|Color Camera |640 x 480 @30 fps |1920 x 1080 @30 fps
|Depth Camera |320 x 240 |512 x 424
|Max Depth Distance |~4.5 M |8 M
|Min Depth Distance |40 cm in near mode |50 cm
|Depth Horizontal Field of View |57 degrees |70 degrees
|Depth Vertical Field of View |43 degrees |60 degrees
|Tilt Motor |yes |no
|Skeleton Joints Defined |20 joints |25 joints
|Full Skeletons Tracked |2 |6
|USB Standard |2.0 |3.0
|======================================================

Table 1: Features of Kinect 1 & 2 (Willy, 2017)

Furthermore the first version of the Kinect is widely used with the ROS
platform. Together with SLAM(Simultaneous Localization And Mapping) it
can be used to make a 3D map of the environment.

image:media/indoornavigation2.jpeg[image,width=434,height=244]

Figure 1: SLAM with Kinect v1

==== LiDAR


The LiDAR is a Light Detecting And Ranging sensor. It uses a laser to
scan the environment. The one which is available for Willy is a SICK-TIM
551 LIDAR. (Sick , 2018)

This specific LiDAR has a field of view of 270 degrees and scans at a
rate of 15Hz. The operation range is between 0.05m up to 10m.

image:media/indoornavigation3.png[image,width=266,height=247]

Figure 2: How a LiDAR works

Together with plotting software, for example RViz, Willy could make a
map which then can be used for indoor navigation. A minor problem with a
LiDAR like the SICK TIM-551 is that it can only create a 2D point cloud.
It will not detect a table when the LiDAR is placed lower for example.

Another task for the LiDAR could be for safety. Because of the LiDAR
being very fast at a 15Hz scanning rate, the SICK TIM-551 can be used
for recognition of possible dangerous obstacles.

image:media/indoornavigation4.jpeg[Afbeeldingsresultaat voor lidar map
rviz,width=555,height=314]

Figure 3: Plotting with RViz

==== Beacons


Beacons are small devices who emits signals. These signals can be
detected by the robot to know where it is located based on signal
strength. The beacons could use Bluetooth, Wi-Fi, radio signals and
there even is a version which is using only light. With triangulation
the distance to the beacons can be measured and the location will be
determined.

image:media/indoornavigation5.png[image,width=516,height=321]

Figure 4: Triangulation for location measuring

==== Ultrasonic Sensors


Ultrasonic sensors are sensors that send and receive sound waves to
measure the distance to an object. It calculates the time between
sending and receiving a wave.

image:media/indoornavigation6.png[image,width=537,height=302]

Figure 5: Working of ultrasonic sensors

There is however a problem when using ultrasonic sensors for mapping the
area. Because the waves of sound are almost randomly cone shaped, the
robot can’t calculate distances as precise as for example LiDAR.

image:media/indoornavigation7.gif[Afbeeldingsresultaat voor ultrasonic sensor
mapping,width=271,height=217]

Figure 6: Cone shaped sensor wave

In this example we see that these sensors could be used for warning
before collision, because the cone only helps improve the coverage of
the area. However for measuring distance and localization they cannot be
used. But more effective for obstacle avoidance and preventing
collisions.

=== Advantages by each solution

==== Kinect

* 3D point cloud
* High-resolution
* People recognition
* ROS integrated and widely documented
* Integrated camera

====  LiDAR

* 2D point cloud
* Fast (15Hz scanning frequency)
* 270 degrees Field of View
* Range (0.05-10m)

==== Beacons

* Cheap (around €30 for three modules)
* Reliable navigation
* High accuracy

* Ultrasonic sensors

* Cheap (< €5 per sensor)
* Easy to set up
=== Disadvantages by each solution

==== Kinect

* Difficult to set up
* Needs further research for implementation
* Small field of view
* Loose its recognition very fast
* Latency

* LiDAR

* Only one height is measured so it could not detect all obstacles (2D)
* Difficult to set up
* The version we have has 270 degrees Field of View while 360 degrees
might be easier to work with

* Beacons

* Not usable without preparation inside the room
* Does not work without a very high amount of beacons

* Ultrasonic sensors

* Easy to fool. When the wave cone hit an object closer to the robot,
the wrong distance is measured.
* Due to the cone shape, measurements are not reliable for mapping and
localization
* More susceptible to interference

Conclusion
----------

For the indoor navigation of Willy, a combination of options can be used.
Because of the documentation which is available for the Kinect V1 and
the fact that these are cheap, we will do a further investigating of
using the Kinect on Willy.

As a second addition Willy can use the SICK TIM-551 LiDAR for safety as
well as for navigation and localization purposes. We will do a further
investigation in the use of this LiDAR.

The Beacons are not a preferred option because preparation of each room
Willy needs to drive is necessary. Beacons make Willy less flexible.

The last suggestion for navigation, the ultrasonic sensors, will not be
used for navigation. The sensors are betters used for obstacle detection
and as a last safety measure for the robot.

=== Bibliography

_Configuration Robot Localization_. (n.d.). Retrieved from
http://docs.ros.org/indigo/api/robot_localization/html/configuring_robot_localization.html
Sick . (2018, 01 19). _Sick-TIM 551_. Retrieved from Sick sensor
intelligence:
https://www.sick.com/us/en/detection-and-ranging-solutions/2d-lidar-sensors/tim5xx/tim551-2050001/p/p343045Willy,
P. 2. (2017). _Research Obstacle Detection V1.2._ Zwolle.
